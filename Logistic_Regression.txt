# TensorFlow and tf.keras
import tensorflow as tf
import keras
from sklearn.decomposition import PCA
import sklearn.metrics 
# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
# import tf_keras
# import tensorflow_probability as tfp
# tfb = tfp.bijectors
# tfd = tfp.distributions
# tfk = tfp.math.psd_kernels

print(tf.__version__)

fashion_mnist = tf.keras.datasets.fashion_mnist
# kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

train_images = train_images / 255.0

test_images = test_images / 255.0

mean = train_images.mean(axis=0)
std = train_images.std(axis=0)
train_images = (train_images - mean) / std
test_images = (test_images - mean) / std

train_images = np.array(train_images)

train_images = train_images.reshape([train_images.shape[0],train_images.shape[2]*train_images.shape[1]])
test_images = test_images.reshape([test_images.shape[0],test_images.shape[2]*test_images.shape[1]])

pca_reduction = 2
learning_rate=.003
iters = 30
reg_rate = .001

experimentAccResults = []
experimentRegRate = []
experimentLossResults = []
experimentLearningRate = []


pca = PCA(pca_reduction)
pca.fit(train_images)
# print(pca.explained_variance_ratio_)
train_images = pca.transform(train_images)
test_images = pca.transform(test_images)
# print(train_images.shape)

# train_images = polynomial_kernel(train_images,degree=2)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(pca_reduction,)),
    tf.keras.layers.Dense(10, activation='softmax')
])
print("reg rate : ",reg_rate)
experimentRegRate.append(reg_rate)
model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(),
            metrics=['accuracy'])


history = model.fit(train_images, train_labels, epochs=iters, validation_split=.2)
probability_model = tf.keras.Sequential([model, 
                                        tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_images)
predictions = np.argmax(predictions, axis=1)
print(predictions)
print("precision: ", sklearn.metrics.precision_score(test_labels,predictions,average='macro'))
print("recall: ",sklearn.metrics.recall_score(test_labels,predictions,average='macro'))

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
acc_matrix = history.history['accuracy']
loss_matrix = history.history['loss']
experimentAccResults.append(test_acc)
experimentLossResults.append(test_loss)

print('\nTest accuracy:', test_acc) 
    



# for i in range(1,10):
#     reg_rate= i*.001
#     model = tf.keras.Sequential([
#     tf.keras.layers.Input(shape=(pca_reduction,)),
#     tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer =tf.keras.regularizers.l1(reg_rate))
# ])
#     print("reg rate : ",reg_rate)
#     experimentRegRate.append(reg_rate)
#     model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate),
#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),
#               metrics=['accuracy'])

    
#     history = model.fit(train_images, train_labels, epochs=iters, validation_split=.2)
#     probability_model = tf.keras.Sequential([model, 
#                                             tf.keras.layers.Softmax()])
#     predictions = probability_model.predict(test_images)
#     predictions = np.argmax(predictions, axis=1)
#     print(predictions)
#     print("precision: ", sklearn.metrics.precision_score(test_labels,predictions,average='macro'))
#     print("recall: ",sklearn.metrics.recall_score(test_labels,predictions,average='macro'))

#     test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
#     acc_matrix = history.history['accuracy']
#     loss_matrix = history.history['loss']
#     experimentAccResults.append(test_acc)
#     experimentLossResults.append(test_loss)
    
#     print('\nTest accuracy:', test_acc) 
    
# np.savetxt('Logistics regression reg results.csv',(experimentRegRate,experimentAccResults,experimentLossResults), delimiter=',')
# try:
#     plt.plot(experimentRegRate,experimentAccResults)
#     plt.savefig("Logistic regression reg rate vs acc.png", dpi=300, bbox_inches='tight')
#     plt.show()
#     plt.plot(experimentRegRate,experimentLossResults)
#     plt.savefig("Logistic regression reg rate vs loss.png", dpi=300, bbox_inches='tight')
#     plt.show()
# except Exception as e:
#     print(e)
    

# print(np.argmax(predictions[0]))
# print(test_labels[0])




# epochs = np.arange(1,iters+1,1)
# print(epochs)
# print(acc_matrix)
# plt.plot(epochs, acc_matrix)
# plt.xlabel("X - Iterations")
# plt.ylabel("Y - Training Accuracy")
# plt.ylim(min(acc_matrix),max(acc_matrix))
# plt.title('LR accuracy iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate)+ ' test accuracy = '+ str(test_acc))
# plt.savefig('LR accuracy iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate) + '.png', dpi=300, bbox_inches='tight')
# plt.show()
# plt.plot(epochs, loss_matrix)
# plt.xlabel("X - Iterations")
# plt.ylabel("Y - Training Loss")
# plt.title('LR loss iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate)+ ' test loss = '+str(test_loss))
# plt.ylim(0,max(loss_matrix))
# plt.savefig('LR loss iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate) + '.png', dpi=300, bbox_inches='tight')
# plt.show()


