# TensorFlow and tf.keras
import tensorflow as tf

import keras
from sklearn.decomposition import PCA
# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import sklearn.metrics 

print(tf.__version__)

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


train_images = train_images / 255.0

test_images = test_images / 255.0

train_images = np.array(train_images)

train_images = train_images.reshape([train_images.shape[0],train_images.shape[2]*train_images.shape[1]])
test_images = test_images.reshape([test_images.shape[0],test_images.shape[2]*test_images.shape[1]])

pca_reduction = 300

pca = PCA(n_components=pca_reduction)
pca.fit(train_images)
print(pca.explained_variance_ratio_)
train_images = pca.transform(train_images)
test_images = pca.transform(test_images)
print(train_images.shape)

reg_rate = .002
iters = 20
learning_rate = .0015
experimentAccResults = []
experimentLossResults = []
experimentLearningRate = []
experimentRegRate = []

# for i in range(1,10):
#     reg_rate = i*.001
#     print("reg rate : ",reg_rate)
    
#     print("learning rate : ",learning_rate)
#     model = tf.keras.Sequential([
#         tf.keras.layers.Dense(pca_reduction),
#         tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True),
#         tf.keras.layers.Dense(64, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
#         # tf.keras.layers.Dense(32, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
#         # tf.keras.layers.Dense(16, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
#         tf.keras.layers.Dense(10)
#     ])

#     model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate),
#                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#                 metrics=['accuracy'])
#     history = model.fit(train_images, train_labels, epochs=iters)

#     test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
#     acc_matrix = history.history['accuracy']
#     loss_matrix = history.history['loss']
#     experimentRegRate.append(reg_rate)
#     experimentAccResults.append(test_acc)
#     experimentLossResults.append(test_loss)
#     print('\nTest accur acy:', test_acc)
# np.savetxt('results NN reg rate.csv',(experimentRegRate,experimentAccResults,experimentLossResults), delimiter=',')
# plt.plot(experimentRegRate,experimentAccResults)
# plt.savefig('accuracy across reg rates.png',dpi=300,bbox_inches='tight')
# plt.show()

model = tf.keras.Sequential([
    tf.keras.layers.Dense(pca_reduction),
    tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True),
    tf.keras.layers.Dense(64, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
    tf.keras.layers.Dense(32, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
    tf.keras.layers.Dense(16, activation='relu',activity_regularizer=tf.keras.regularizers.l2(reg_rate)),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy'])
history = model.fit(train_images, train_labels, epochs=iters)

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
probability_model = tf.keras.Sequential([model, 
                                            tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_images)
predictions = np.argmax(predictions, axis=1)
print(predictions)
print("precision: ", sklearn.metrics.precision_score(test_labels,predictions,average='macro'))

print("recall: ",sklearn.metrics.recall_score(test_labels,predictions,average='macro'))
acc_matrix = history.history['accuracy']
loss_matrix = history.history['loss']
experimentRegRate.append(reg_rate)
experimentAccResults.append(test_acc)
experimentLossResults.append(test_loss)
print('\nTest accur acy:', test_acc)




epochs = np.arange(1,iters+1,1)
print(epochs)
print(acc_matrix)
plt.plot(epochs, acc_matrix)
plt.xlabel("X - Iterations")
plt.ylabel("Y - Training Accuracy")
plt.ylim(min(acc_matrix),max(acc_matrix))
plt.title('PCA NN accuracy iters = '+ str(iters)  + ' learning_rate = '+ str(learning_rate) + ' PCA reduction = ' + str(pca_reduction) + ' reg = ' + str(reg_rate) + ' test accuracy = '+str(test_acc))
plt.savefig('PCA test NN accuracy iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate) + '.png', dpi=300, bbox_inches='tight')
plt.show()
plt.plot(epochs, loss_matrix)
plt.xlabel("X - Iterations")
plt.ylabel("Y - Training Loss")
plt.ylim(0,max(loss_matrix))

plt.title('PCA NN loss iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate)+ ' test loss = '+str(test_loss))
plt.savefig('PCA test NN loss iters '+ str(iters)  + ' learning_rate '+ str(learning_rate) + ' PCA reduction ' + str(pca_reduction) + ' reg ' + str(reg_rate) + '.png', dpi=300, bbox_inches='tight')
plt.show()


